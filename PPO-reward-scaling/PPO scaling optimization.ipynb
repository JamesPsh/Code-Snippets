{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PPO scaling optimization'''\n",
    "'''https://arxiv.org/pdf/2005.12729.pdf'''\n",
    "\n",
    "'''\n",
    "R = 0\n",
    "RS = []\n",
    "while not done:\n",
    "    s, r, ... = env.step(action)\n",
    "    R = R * GAMMA + r\n",
    "    RS.append(R)\n",
    "\n",
    "    scaled_reward = r / np.std(RS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6344526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RewardScaling:\n",
    "    def __init__(self, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.init_statistics()\n",
    "        \n",
    "\n",
    "    def init_statistics(self):\n",
    "        self.iter = 0\n",
    "        self.R = 0\n",
    "        self.E_x_squared = 0\n",
    "        self.E_x = 0\n",
    "        \n",
    "    \n",
    "    def update_mean(self, E_x, x):\n",
    "        E_x *= (1 - (1 / self.iter))\n",
    "        E_x += (x / self.iter)\n",
    "        return E_x\n",
    "    \n",
    "    \n",
    "    def update_statistics(self, reward):\n",
    "\n",
    "        self.iter += 1\n",
    "\n",
    "        # tracking E(x), E(x ** 2) for V(x)\n",
    "        # V(x) = E(x ** 2) - E(x) ** 2\n",
    "\n",
    "        # R = R * gamma + r\n",
    "        self.R *= self.gamma\n",
    "        self.R += reward\n",
    "\n",
    "        # E(x ** 2)\n",
    "        self.E_x_squared = self.update_mean(self.E_x_squared, self.R ** 2)\n",
    "\n",
    "        # E(x)\n",
    "        self.E_x = self.update_mean(self.E_x, self.R)\n",
    "\n",
    "        \n",
    "    # for scale observations\n",
    "    def get_norm_factor(self):\n",
    "\n",
    "        # V(x) = E(x ** 2) - E(x) ** 2\n",
    "        V_x = self.E_x_squared - self.E_x ** 2\n",
    "\n",
    "        # SD(x) = V(x) ** 0.5\n",
    "        return np.sqrt(V_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5bfb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "max_count = 10000\n",
    "gamma = 0.95\n",
    "rs = RewardScaling(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a4af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.02803778648376465 value: 57733.716668208566\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "list_std = []\n",
    "for reward in range(0, max_count):\n",
    "    rs.update_statistics(reward)\n",
    "    list_std.append(rs.get_norm_factor())\n",
    "\n",
    "end = time()\n",
    "print('time:', end - start, 'value:', list_std[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e809f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.606989622116089 value: 57733.7166682084\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "RS = []\n",
    "R = 0\n",
    "list_std = []\n",
    "for reward in range(0, max_count):\n",
    "    R = R * gamma + reward\n",
    "    RS.append(R)\n",
    "    list_std.append(np.std(RS))\n",
    "\n",
    "end = time()\n",
    "print('time:', end - start, 'value:', list_std[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
